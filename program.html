<!DOCTYPE HTML>


<!--
 Berkeley Vision and Learning Center (BVLC)
 
 Design based on:
 Strongly Typed 1.0 by HTML5 UP
 html5up.net | @n33co
 Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
 -->
<html>
    <head>
        <title>WiCV</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <meta name="viewport" content="width=1040" />
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600|Arvo:700" rel="stylesheet" type="text/css" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.dropotron.js"></script>
        <script src="js/jquery.dotdotdot.min.js"></script>
        <script src="js/config.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-panels.min.js"></script>
        <script src="js/jquery.slides.min.js"></script>
        <link rel="stylesheet" href="css/font-awesome.min.css">
            <noscript>
                <link rel="stylesheet" href="css/style.css" />
                <link rel="stylesheet" href="css/style-desktop.css" />
                <link rel="stylesheet" href="css/skel-noscript.css" />
            </noscript>
        
        <style>
        
        /* Style the tab */
        div.tab {
            margin: auto;
            width: 75%;
            padding: 0px 12px; 
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
        
        /* Style the buttons inside the tab */
        div.tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
            font-size: 17px;
        }
        
        /* Change background color of buttons on hover */
        div.tab button:hover {
            background-color: #ddd;
        }
        
        /* Create an active/current tablink class */
        div.tab button.active {
            background-color: #ccc;
        }
        
        /* Style the tab content */
        .tabcontent {
            margin: auto;
            width: 75%;
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
        </style>

            </head>
    <body class="homepage">
        <!-- Header Wrapper -->
        <div id="header-wrapper">
            <!-- Header -->
            <div id="header" class="container">
                <!-- Logo -->
                <!--<h1 id="logo">
                 <a id="home" href="#"></a>
                 </h1>-->
                <!-- Nav -->
                <nav id="nav">
                    <ul style="padding-top: 10px; padding-bottom: 2em">
                        <li>
                            <a href="index.html#header" class="">
                                <span style="
                                    position: relative;
                                    width: 261px;
                                    display: inline-block;
                                    height: 10px;
                                    ">
                                    <img src="images/wicv_logo_simple.png" style="
                                    position: absolute;
                                    left: 0;
                                    width: 111px;
                                    top: -20px;
                                    "/>
                                </span>
                            </a>
                        </li>
                        
                        <li>
                            <a href="index.html#header"><span>Home</span></a>
                        </li>
                        <li>
                            <a href="program.html"><span>Program</span></a>
                        </li>
                        <li>
                            <a href="faq.html"><span>FAQ</span></a>
                        </li>
                        <li>
                            <a href="committee.html"><span>Committee</span></a>
                        </li>
                        <li>
                            <a href="participation.html"><span>Call for Participation</span></a>
                        </li>
                        <li>
                            <a href="contact.html"><span>Contact</span></a>
                            
                        </li>
                        <li>
                           <a href="#"><span>WiCV</span></a>
                           <ul>
                              <li><a href="https://wicvworkshop.github.io/ECCV2018/index.html">WiCV @ECCV 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/index.html">WiCV @CVPR 2018</a></li>
                              <li><a href="https://wicvworkshop.github.io/2017/index.html">WiCV 2017</a></li>
                              <li><a href="https://sites.google.com/site/wicv2016/home">WiCV 2016</a></li>
                              <li><a href="https://sites.google.com/site/wicv2015/home">WiCV 2015</a></li>
                           </ul>
                        </li>
                    </ul>
                </nav>
            </div>
        </div>
        
        <div class="banner-wrapper">
            <div class="inner">
                <!-- Banner -->
                <section class="banner container">
                    <br>
                    <h2 id="faculty">Program</h2>
                    
                </section>
            </div>
        </div>
        <div class="features-wrapper">
                  
        <p style="color:red;"><strong>* The final schedule will be announced here. * </strong></p>

        <div class="tab">
          <button class="tablinks" onclick="openCity(event, 'Schedule')" id="defaultOpen">Schedule</button>
          <button class="tablinks" onclick="openCity(event, 'Talks')">Talks</button>
          <button class="tablinks" onclick="openCity(event, 'Panel')">Panel</button>
          <button class="tablinks" onclick="openCity(event, 'Orals')">Orals</button>
          <button class="tablinks" onclick="openCity(event, 'Posters')">Posters</button>
          <button class="tablinks" onclick="openCity(event, 'Dinner')">Dinner</button>
        </div>
        
        <div id="Schedule" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Main Workshop on June 22</h3>
          <!--<h5 style="color:red;font-style:italic; margin: 0px 0px 20px 0px;">tentative schedule</h5> -->
          <h5 style="color:red; margin: 0px 0px 20px 0px;">Room: TBA</h5>
           
          <p style="margin: 0px 0px 20px 40px;text-align:left;">
                8:50  - 9:00  &emsp;&emsp;&emsp;<b>Introduction</b> <br>
                9:00  - 9:30  &emsp;&emsp;&emsp;<b>Invited Talk1</b><br>                
                9:30  - 10:00 &ensp;&emsp;&emsp;<b>Invited Talk2</b><br>
                10:00 - 11:30 &emsp;&emsp;<b>Poster Session and Morning Break</b><br>
                11:30 - 11:50  &emsp;&emsp;<b> Oral Session1</b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Learnable PINs: Cross-Modal Embeddings for Person Identity, by Arsha Nagrani (Oxford University)</a><br>
                11:50 - 12:10  &emsp;&emsp;<b> Oral Session2 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification, by Sima
Behpour (University of Illinois) </a><br>
                12:10 - 13:30  &emsp;&emsp;<b> Lunch </b><br>
                13:30 - 14:00  &emsp;&emsp;<b> Invited Talk3 </b><br>
                14:00 - 14:20  &emsp;&emsp;<b> Oral Session3 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering, by Aishwarya Agrawal (Georgia Tech) </a><br>
                14:20 - 14:50  &emsp;&emsp;<b> Oral Session4 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;On the iterative refinement of densely connected representation levels for semantic segmentation, by Arantxa Casanova (MILA) </a><br>
                14:50 - 15:20  &emsp;&emsp;<b> Invited Talk4 </b><br>
                15:20 - 15:40  &emsp;&emsp;<b> Oral Session5 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Gradient-free policy architecture search and adaptation, by Sayna Ebrahimi (UC Berkeley) </a><br>
                15:40 - 16:00  &emsp;&emsp;<b> Oral Session6 </b><br>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Joint Event Detection and Description in Continuous Video Streams, by Huijuan Xu (Boston University) </a><br>
                16:00 - 16:30  &emsp;&emsp;<b> Afternoon Break </b><br>
                16:30 - 17:10  &emsp;&emsp;<b> Panel </b><br>
                17:10 - 17:20  &emsp;&emsp;<b>Closing Remarks</b> <br>
          </p>

        </div>
        
        <div id="Talks" class="tabcontent">
          <h2 style="margin: 20px 0px 0px 0px;">Keynote Talks</h2>
          <p>Keynote speakers will give technical talks about their research in computer vision.</p> 

          <!-- Jessica K. Hodgins -->  
         
          <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.cmu.edu/~jkh/" class="image image-centered"><img height=230 width=120 src="images/speakers/jessicaHodings.jpg" alt="" /></a>
                <strong> Jessica K. Hodgins (Carnegie Mellon University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> Jessica Hodgins is a Professor in the Robotics Institute and Computer Science Department at Carnegie Mellon University. From 2008-2016, she founded and ran research labs for Disney, rising to VP of Research and leading the labs in Pittsburgh and Los Angeles. From 2005-2015, she was Associate Director for Faculty in the Robotics Institute, running the promotion and tenure process and creating a mentoring program for pre-tenure faculty. Prior to moving to Carnegie Mellon in 2000, she was an Associate Professor and Assistant Dean in the College of Computing at Georgia Institute of Technology. She received her Ph.D. in Computer Science from Carnegie Mellon University in 1989. Her research focuses on computer graphics, animation, and robotics with an emphasis on generating and analyzing human motion. She has received a NSF Young Investigator Award, a Packard Fellowship, and a Sloan Fellowship. She was editor-in-chief of ACM Transactions on Graphics from 2000-2002 and ACM SIGGRAPH Papers Chair in 2003. She was an elected director at large on the ACM SIGGRAPH Executive Committee from 2012-2017 and in 2017 she was elected ACM SIGGRAPH President. In 2010, she was awarded the ACM SIGGRAPH Computer Graphics Achievement Award and in 2017 she was awarded the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics.
                  </p> 
              </div>
            </div>    

          <div class="row">
            <div class="4u"> 
                <a class="image image-centered"><img height=230 width=120 src="images/speakers/LauraLeal.png" alt="" /></a>
                <strong> Laura Leal-Taixé (Technical University of Munich)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Bio:</b> Prof. Laura Leal-Taixé is leading the Dynamic Vision and Learning group at the Technical University of Munich, Germany. 
She received her Bachelor and Master degrees in Telecommunications Engineering from the Technical University of Catalonia (UPC), Barcelona. She did her Master Thesis at Northeastern University, Boston, USA and received her PhD degree (Dr.-Ing.) from the Leibniz University Hannover, Germany. 
During her PhD she did a one-year visit at the Vision Lab at the University of Michigan, USA. 
She also spent two years as a postdoc at the Institute of Geodesy and Photogrammetry of ETH Zurich, Switzerland and one year at the Technical University of Munich.
Her research interests are dynamic scene understanding, in particular multiple object tracking and segmentation, as well as machine learning for video analysis.
                  </p> 
              </div>
            </div>       

                  

          <!-- Cordelia Schmid -->  
<!--          
          <div class="row">
            <div class="4u"> 
                <a href="http://thoth.inrialpes.fr/~schmid/" class="image image-centered"><img height=230 width=120 src="images/speakers/cordeliaschmid.jpg" alt="" /></a>
                <strong>Cordelia Schmid (INRIA)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                    <b>Title:</b> Learning to Segment Moving Objects
                    <br><br>
                    <b>Abstract:</b> This talk addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a “visual memory” in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given video frames as input, our approach first assigns each pixel an object or background label obtained with an encoder-decoder network that takes as input optical flow and is trained on synthetic data. Next, a “visual memory” specific to the video is acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results.
                    <br><br>
                    <b>Bio:</b> Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis received the best thesis award from INPG in 1996. Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at INRIA Grenoble Rhone-Alpes, where she is a research director and directs an INRIA team. Dr. Schmid is the author of over a hundred technical publications. She has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), editor-in-chief for IJCV (2013---), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015 and ECCV 2020. In 2006, 2014 and 2016, she was awarded the Longuet-Higgins prize for fundamental contributions in computer vision that have withstood the test of time. She is a fellow of IEEE. She was awarded an ERC advanced grant in 2013, the Humbolt research award in 2015 and the Inria & French Academy of Science Grand Prix in 2016. She was elected to the German National Academy of Sciences, Leopoldina, in 2017.
                  </p> 
              </div>
            </div>          
-->
        </div> <!-- Talks end --> 
        
        <div id="Panel" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Panel</h3>
          <p>Panelists will answer questions and discuss about increasing diversity in computer vision.</p>
          <!-- <p>Feel free to ask your anonymous questions <a href="https://docs.google.com/forms/d/1BkdGAhHXcJl0ApC8lDNkmpEY04S312nldCVC0Ej09jY/">here</a>.</p> -->

        
            <!-- Helge Seetzen  --> 
<!-- 
            <div class="row">
            <div class="4u"> 
                <a href="http://www.tandemlaunch.com" class="image image-centered"><img height=230 width=120 src="images/speakers/helgeseetzen.png" alt="" /></a>
                <strong>Helge Seetzen (TandemLaunch)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 Helge is an award-winning technologist, entrepreneur, and a recognized global authority on technology transfer and display technologies. As General Partner of TandemLaunch, he works with inventors and entrepreneurs to build high growth technology companies. His past successes include the transformation of raw university IP into fully commercialized LED TV technology, including selling his last company - Brightside Technologies - to Dolby Laboratories after sealing partnerships with several of the largest consumer electronics companies in the world. Helge holds over 80 patents in the fields of display, camera and video technology.
                  </p> 
              </div>
            </div> 
-->         
        </div> <!-- Panel end --> 

        <div id="Orals" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Oral Presentations</h3>
          <p>A few accepted papers are invited to give oral presentations.</p>

          <p style="text-align:left"><b>Presenter instructions: </b> TBA.</p>   
          <div class="divider">
              <hr class="left"/><b>Accepted orals</b><hr class="right" />
          </div>
          <div class="row" style="text-align:left"></div>
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u"> <b>Presenter Name</b> </div><div class="2u">  <b>Institution</b> </div><div class="7u">  <b>Paper Title</b> </div></div>  
      
          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Arantxa Casanova </div><div class="2u">  MILA  </div><div class="7u">  On the iterative refinement of densely connected representation levels for semantic segmentation </div></div>


          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Huijuan Xu  </div><div class="2u">  University of Boston  </div><div class="7u">  Joint Event Detection and Description in Continuous Video Streams  </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sima Behpour </div><div class="2u">  University of Illinois </div><div class="7u">   ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Arsha Nagrani </div><div class="2u">  University of Oxford </div><div class="7u">  Learnable PINs: Cross-Modal Embeddings for Person Identity  </div></div>

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Sayna Ebrahimi </div><div class="2u">  UC Berkeley </div><div class="7u">  Gradient-free policy architecture search and adaptation  </div></div>        

          <div class="row" style="text-align:left"><div class="1u"> </div><div class="2u">  Aishwarya Agrawal </div><div class="2u">  Georgia Tech </div><div class="7u">  Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering  </div></div>


        </div>


        <div id="Posters" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Poster Presentations</h3>
          <p>Authors of all accepted papers (with or without travel grant) will present their work in a poster session.</p>                

          <p style="text-align:left"><b>Presenter instructions: TBA
<!-- </b>All posters should be installed in at most 10 minutes at the start of the poster session in the afternoon. The poster boards are located at the room Kamahameha II (where the main conference posters were). The physical dimensions of the poster stands are 8 feet wide by 4 feet high. Poster presenters can optionally use the <a href="http://cvpr2017.thecvf.com/files/cvpr17_poster_template_n.pptx"> CVPR18 poster template</a> for more details on how to prepare their posters. Please note your poster number below to find your board.</p>  
--> 
          <div class="divider">
              <hr class="left"/><b>Accepted Posters</b><hr class="right" />
          </div><br>

<div class="row" style="text-align:left"><div class="1u"> No </div><div class="2u">  Presenter Name </div><div class="2u">  Institution </div><div class="7u">  Paper Title </div></div>
      <hr/> 
 <br>

<div class="row" style="text-align:left"><div class="1u"> 1 </div><div class="2u">  Doris Antensteiner </div><div class="2u">  Svorad Stolc Austrian Institute of Technology </div><div class="7u"> Variational Depth and Normal Fusion Algorithms for 3D Reconstruction  </div></div>
<div class="row" style="text-align:left"><div class="1u"> 2 </div><div class="2u"> Mengjiao Wang </div><div class="2u"> Imperial College London   </div><div class="7u">  A Neuro-Tensorial Approach For Learning Disentangled Representations </div></div>
<div class="row" style="text-align:left"><div class="1u"> 3 </div><div class="2u"> Ksenia Bittner </div><div class="2u">   German Aerospace Center   </div><div class="7u">  Automatic Large-Scale 3D Building Shape Refinement Using Conditional Generative Adversarial Networks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 4 </div><div class="2u">  Franziska Mueller </div><div class="2u">   MPI Informatics   </div><div class="7u">  GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB </div></div>
<div class="row" style="text-align:left"><div class="1u"> 5 </div><div class="2u"> Yanran Joyce Wang </div><div class="2u">  Northwestern University  </div><div class="7u">  Quick Adaption of Segmentation FCN via Network Modulation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 6 </div><div class="2u"> Derya Akkaynak </div><div class="2u">  University of Haifa  </div><div class="7u">  A Revised Underwater Image Formation Model </div></div>
<div class="row" style="text-align:left"><div class="1u"> 7 </div><div class="2u"> Dena Bazazian </div><div class="2u">  CVC  </div><div class="7u">  Word Spotting in Scene Images based on Character Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 8 </div><div class="2u"> Tamar Rott Shaham </div><div class="2u">  Technion  </div><div class="7u">  Deformation Aware Image Compression </div></div>
<div class="row" style="text-align:left"><div class="1u"> 9 </div><div class="2u"> Meng Zheng </div><div class="2u">  Rensselaer Polytechnic Institute  </div><div class="7u">  RPIfield: A New Dataset for Temporally Evaluating Person Re-Identification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 10 </div><div class="2u"> Ilke Demir </div><div class="2u">  Facebook  </div><div class="7u">  A Holistic Framework for Addressing the World using Machine Learning </div></div>
<div class="row" style="text-align:left"><div class="1u"> 11 </div><div class="2u"> Bojana Gajic </div><div class="2u">  Computer Vision Center  </div><div class="7u">  Cross-domain fashion image retrieval </div></div>
<div class="row" style="text-align:left"><div class="1u"> 12 </div><div class="2u"> Simone Meyer </div><div class="2u">  ETH Zurich  </div><div class="7u">  PhaseNet for Video Frame Interpolation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 13 </div><div class="2u"> Jingya Liu </div><div class="2u">  City College of New York  </div><div class="7u">  Recognizing Elevator Buttons and Labels for Blind Navigation </div></div>
<!-- <div class="row" style="text-align:left"><div class="1u"> 14 </div><div class="2u"> Amanda Duarte </div><div class="2u">  Universitat Politecnica de Catalunya  </div><div class="7u">  Cross-modal Embeddings for Video and Audio Retrieval </div></div> -->
<div class="row" style="text-align:left"><div class="1u"> 14 </div><div class="2u">   Kuan-Ting Chen </div><div class="2u">  National Taiwan University  </div><div class="7u">   Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures </div></div>
<div class="row" style="text-align:left"><div class="1u"> 15 </div><div class="2u"> Ruth Fong </div><div class="2u">  University of Oxford  </div><div class="7u">  Net2Vec: Explaining how Concepts are Encoded in Deep Neural Networks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 16 </div><div class="2u"> Ozge Yalcinkaya </div><div class="2u">  Hacettepe University  </div><div class="7u">  I-ME: Iterative Model Evolution for Learning Activities From Weakly Labeled Videos
 </div></div>
<div class="row" style="text-align:left"><div class="1u"> 17 </div><div class="2u"> Kanami Yamagishi </div><div class="2u">  Waseda University  </div><div class="7u">  Cosmetic Features Extraction by a Single Image Makeup Decomposition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 18 </div><div class="2u"> Rosaura Vidal Mata  </div><div class="2u">  University of Notre Dame  </div><div class="7u">  UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition </div></div>
<div class="row" style="text-align:left"><div class="1u"> 19 </div><div class="2u"> Avantika Singh </div><div class="2u">  IIT Mandi  </div><div class="7u">  Encapsulating the impact of transfer learning, domain knowledge and training strategies in deep-learning based architecture: A biometric based case study </div></div>
<div class="row" style="text-align:left"><div class="1u"> 20 </div><div class="2u"> Jadisha Ramirez Cornejo </div><div class="2u">  University of Campinas  </div><div class="7u">  Dynamic Facial Expression Recognition through Visual Rhythm and Motion History Image </div></div>
<div class="row" style="text-align:left"><div class="1u"> 21 </div><div class="2u"> Karla Brkic </div><div class="2u">  University of Zagreb  </div><div class="7u">  Keep it Short: Understanding Traffic Scenes From Very Short Representations </div></div>
<div class="row" style="text-align:left"><div class="1u"> 22 </div><div class="2u"> Hiya Roy </div><div class="2u">  University of Tokyo  </div><div class="7u">  Do hashtags help? - Image aesthetics prediction using only hashtags </div></div>
<div class="row" style="text-align:left"><div class="1u"> 23 </div><div class="2u"> Marcella Cornia </div><div class="2u">  University of Modena and Reggio Emilia  </div><div class="7u">  SAM: Pushing the Limits of Saliency Prediction Models </div></div>
<div class="row" style="text-align:left"><div class="1u"> 24 </div><div class="2u"> Yi Zhu </div><div class="2u">  University of Chinese Academy of Sciences  </div><div class="7u">  To Be Focused: Efficient Weakly Supervised Learning via Soft Proposal Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 25 </div><div class="2u"> Nezihe Merve Gürel </div><div class="2u">  ETH Zürich  </div><div class="7u">  Towards More Accurate Radio Telescope Images </div></div>
<!-- <div class="row" style="text-align:left"><div class="1u"> 27 </div><div class="2u"> Moi Hoon Yap </div><div class="2u">  MMU  </div><div class="7u">  Facial Wrinkles Annotator: Coarse and fine wrinkles </div></div> -->
<div class="row" style="text-align:left"><div class="1u"> 26 </div><div class="2u"> Yi Zhu </div><div class="2u">  University of Chinese Academy of Sciences  </div><div class="7u">  Learning Peak Response for Weakly Supervised Instance-level Segmentation </div></div>
<div class="row" style="text-align:left"><div class="1u"> 27 </div><div class="2u"> Mahdieh Poostchi </div><div class="2u">  NIH  </div><div class="7u">  
Multi-scale Spatially weighted Local Histogram in O(1) </div></div>
<div class="row" style="text-align:left"><div class="1u"> 28 </div><div class="2u"> Mahdieh Poostchi </div><div class="2u">  NIH  </div><div class="7u">  Malaria Parasite Detection and Quantification Using Deep Neural Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 29 </div><div class="2u"> Mikayla Timm </div><div class="2u">  University of Massachusetts Amherst  </div><div class="7u">  Large-Scale Ecological Analyses of Animals in the Wild using Computer Vision </div></div>
<div class="row" style="text-align:left"><div class="1u"> 30 </div><div class="2u"> Ekta Gujral </div><div class="2u">  University of California, Riverside   </div><div class="7u">  FMVR:Shall I Get My Video Back? Feature Matching based Video Reconstruction </div></div>
<div class="row" style="text-align:left"><div class="1u"> 31 </div><div class="2u"> Murium Iqbal </div><div class="2u">  Overstock  </div><div class="7u">  Discovering Style Trends through Deep Visually Aware Latent Product Embeddings </div></div>
<div class="row" style="text-align:left"><div class="1u"> 32 </div><div class="2u"> Anis Davoudi </div><div class="2u">  university of florida  </div><div class="7u">  
Autonomous detection of disruptions in the intensive care unit using deep mask R-CNN </div></div>
<div class="row" style="text-align:left"><div class="1u"> 33 </div><div class="2u"> Jing Zhang </div><div class="2u">  Australian National University  </div><div class="7u">  
Deep Saliency Detection: From Supervised Learning to Unsupervised Learning </div></div>
<div class="row" style="text-align:left"><div class="1u"> 34 </div><div class="2u"> Akram Bayat </div><div class="2u">  UMass Boston  </div><div class="7u">  
Multi-Resolution Deep Object Recognition Network </div></div>
<div class="row" style="text-align:left"><div class="1u"> 35 </div><div class="2u"> Fariba Zohrizadeh </div><div class="2u">  University of Texas at Arlington  </div><div class="7u">  Image Segmentation using Sparse Subset Selection </div></div>
<div class="row" style="text-align:left"><div class="1u"> 36 </div><div class="2u"> Ivona Tautkute </div><div class="2u">  Tooploox  </div><div class="7u">  
I Know How You Feel: Emotion Recognition with Facial Landmarks </div></div>
<div class="row" style="text-align:left"><div class="1u"> 37 </div><div class="2u"> Jyoti Islam </div><div class="2u">  Georgia State University  </div><div class="7u">  
Early Diagnosis of Alzheimer's Disease: A Neuroimaging Study with Deep Learning Architectures </div></div>
<div class="row" style="text-align:left"><div class="1u"> 38 </div><div class="2u"> Sheila Pinto Caceres </div><div class="2u">  University of Sydney  </div><div class="7u">  
Activity Recognition under Energy Constraints </div></div>
<div class="row" style="text-align:left"><div class="1u"> 39 </div><div class="2u"> vibha gupta </div><div class="2u">  IIT Mandi  </div><div class="7u">  
Hybridization of Feature Selection Methods Based on Tournament Design: HEp-2 Cell Image Classification </div></div>
<div class="row" style="text-align:left"><div class="1u"> 40 </div><div class="2u"> Reham Abobeah </div><div class="2u">  Egypt Japan University of Science and Technology  </div><div class="7u">  Wearable RGB Camera-based Navigation System for the Visually Impaired </div></div>
<div class="row" style="text-align:left"><div class="1u"> 41 </div><div class="2u"> Rajvi Shah </div><div class="2u">  IIIT Hyderabad  </div><div class="7u">  
View-graph Selection Framework for SfM </div></div>





        </div>

        <div id="Dinner" class="tabcontent">
          <h3 style="margin: 20px 0px 0px 0px;">Mentoring Dinner on June 21 </h3>
          <!-- <p style="font-style:italic;">by invitation only</p> -->
          <p>6:30 - 10:00 pm   Dinner sponsored by Facebook</p> 
          <p align="left";>The dinner event is an opportunity to meet other female computer vision researchers. Poster presenters will be matched with senior computer vision researchers to share experience and career advice. Invitees will receive an e-mail and be asked to confirm attendance.</p>
          <p align="left";><b>*Note that the dinner takes place the evening before the main workshop day.*</b></p>  
         
          <div class="divider">
              <hr class="left"/><b>Dinner speakers</b><hr class="right" />
          </div><br>

        <!-- Dima Damen -->
            <div class="row">
            <div class="4u"> 
                <a href="https://dimadamen.github.io/" class="image image-centered"><img height=230 width=120 src="images/speakers/DimaDamen.jpg" alt="" /></a>
                <strong>Dima Damen (University of Bristol)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                 Dima Damen is a Senior Lecturer (Associate Professor) in Computer Vision at the University of Bristol, United Kingdom. Received her PhD from the University of Leeds (2009). Dima's research interests are in the automatic understanding of object interactions, actions and activities using static and wearable visual (and depth) sensors. Dima co-chaired BMVC 2013, is area chair for BMVC (2014-2018), associate editor of Pattern Recognition (2017-). She was selected as a Nokia Research collaborator in 2016, and as an Outstanding Reviewer in ICCV17, CVPR13 and CVPR12. She currently supervises 9 PhD students, and 3 postdoctoral researchers. 
                  </p> 
              </div>
            </div> 

 
            <!-- OLGA RUSSAKOVSKY --> 
<!--
            <div class="row">
            <div class="4u"> 
                <a href="http://www.cs.princeton.edu/~olgarus/index.html" class="image image-centered"><img height=230 width=120 src="images/speakers/olgarussakovsky.jpg" alt="" /></a>
                <strong>Olga Russakovsky (Princeton University)</strong>
              </div>      
              <div class="8u">
                <p style="text-align:justify;font-size:0.75em;line-height: 1.2;">
                  Olga Russakovsky is an Assistant Professor of Computer Science at Princeton University. She completed her PhD in Computer Science at Stanford University in August 2015 and her postdoc at the Robotics Institute of Carnegie Mellon University in June 2017. Her research is in computer vision, closely integrated with machine learning and human-computer interaction. Her work was featured in the New York Times and MIT Technology Review. She served as a Senior Program Committee member for WACV’16 (and CVPR’18 soon), led the ImageNet Large Scale Visual Recognition Challenge effort for two years, was the Publicity and Press chair at CVPR’16, and organized multiple workshops and tutorials on large-scale recognition at premier computer vision conferences ICCV’13, ECCV’14, CVPR’15, ICCV’15, CVPR’16, ECCV’16 and CVPR’17. In addition, she was the co-founder and director of the Stanford AI Laboratory’s outreach camp SAILORS (featured in Wired and published in SIGCSE’16) which educates high school girls about AI, and is the co-founder and board member of the AI4ALL foundation dedicated to cultivating a diverse group of future AI leaders. 
                  </p> 
              </div>
            </div>  
-->

         
         </div>

        <script>
           function openCity(evt, cityName) {
               var i, tabcontent, tablinks;
               tabcontent = document.getElementsByClassName("tabcontent");
               for (i = 0; i < tabcontent.length; i++) {
                   tabcontent[i].style.display = "none";
               }
               tablinks = document.getElementsByClassName("tablinks");
               for (i = 0; i < tablinks.length; i++) {
                   tablinks[i].className = tablinks[i].className.replace(" active", "");
               }
               document.getElementById(cityName).style.display = "block";
               evt.currentTarget.className += " active";
           }
           
           // Get the element with id="defaultOpen" and click on it
           document.getElementById("defaultOpen").click();
        </script>

          <br><br><br><br><br> 
        </div>
        <!-- Footer Wrapper -->
        <div class="footer">
            <div class="container">
                    <div class="row">
                            <div class="7u">
             <p>&copy; WiCV 2018</p>
            </div>
            <div class="1u">
                   <a href="mailto:wicv18-organizers@googlegroups.com" class="fa fa-envelope" style="font-size:1.25em;" data-placement="top" data-toggle="tooltip" title="Email"></a>
                       </div>
                       <div class="1u">                       
                                                        <a href="https://twitter.com/wicvworkshop" class="fa fa-twitter" data-placement="top" style="font-size:1.25em;"  data-toggle="tooltip" title="Twitter"></a>
                                                     </div>
                                                         <div class="1u">
                                                               <a href="https://www.facebook.com/WomenInComputerVision/" class="fa fa-facebook" data-placement="top" style="font-size:1.25em;" data-toggle="tooltip" title="Facebook"></a>
                                                               </div>
                                                           </div>
                                                           </div>
                                                         </div>
        <script src="//static.getclicky.com/js" type="text/javascript"></script>
        <script type="text/javascript">try{ clicky.init(100926441); }catch(e){}</script>
        <noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100926441ns.gif" /></p></noscript>
    </body>
</html>
